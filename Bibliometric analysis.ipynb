{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliometric analysis on DAOs\n",
    "In order to perform this bibliometric analysis we will look at documents from Scopus, WebOfScience and Science Direct, and we will analyze co-citation, bibliographic coupling and co-authoring networks regarding papers related to DAOs and decentralized organizations from the fields of business and computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scopus and ScienceDirect api initialization\n",
    "We will start by initializing the API for Scopus and ScienceDirect, and set up a search strategy to recall articles with given keywords in their title or abstracts from the fields of interests.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "    \n",
    "## Load configuration\n",
    "con_file = open(\"config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()\n",
    "\n",
    "## Initialize client\n",
    "client = ElsClient(config['apikey'])\n",
    "#client.inst_token = config['insttoken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our study began by performing a set of searches on Semantic Scholar and Google Scholar, employing the following \"climate AND dao*\", \"decentralized AND autonomous AND organiz*\", \"dao* AND governance\", \"dao* AND web*\", \"web* AND decentraliz*.<br>\n",
    "We then widened our research scope in two ways: the first was to include articles from Google Scholar and Semantic Scholar retrieved through the keywords \"p2p AND dao*\", \"peer-to-peer AND dao*\", \"p2p AND blockchain AND organiz*\", and \"peer-to-peer AND blockchain AND organiz*\".; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we initialize our series of keywords\n",
    "In order to perform our research we must have a list of keywords we want to search using the apis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list=[\"climate AND dao*\", \"decentralized AND autonomous AND organiz*\", \"dao* AND governance\", \"dao* AND web*\", \"web* AND decentraliz*\", \"p2p AND dao*\", \"peer-to-peer AND dao*\", \"p2p AND blockchain AND organiz*\", \"peer-to-peer AND blockchain AND organiz*\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We aggregate papers published in the business domain\n",
    "First of all we create a Pandas dataframe aggregating papers retrieved with the aforementioned keywords in the business domain. In the DF we memorize the title, author(s), year of publication and SCopusID of each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df=pd.DataFrame(columns=['Author(s)', 'Title', 'YearPublished', 'DOI', 'ScopusID'])\n",
    "\n",
    "for key in keyword_list:\n",
    "    #we print the keyword as an indicator of the progress\n",
    "    #print(key)\n",
    "    doc_srch = ElsSearch(\"TITLE-ABS-KEY(\"+key+\") AND SUBJAREA(BUSI OR DECI OR ECON OR SOCI)\",'scopus')\n",
    "    doc_srch.execute(client, get_all = True)\n",
    "\n",
    "    for el in doc_srch.results:\n",
    "        try:\n",
    "            data=[el[\"dc:creator\"],el[\"dc:title\"],el[\"prism:coverDate\"], el[\"prism:doi\"], el[\"dc:identifier\"].split(\":\")[1]]\n",
    "        except Exception as e:\n",
    "            data=[]\n",
    "            input_elements=[\"dc:creator\", \"dc:title\", \"prism:coverDate\", \"prism:doi\", \"dc:identifier\"]\n",
    "            for field in input_elements:\n",
    "                try:\n",
    "                    data.append(el[field])\n",
    "                except Exception as f:\n",
    "                    data.append(\"NaN\")\n",
    "\n",
    "        business_df.loc[len(business_df), :] = data\n",
    "\n",
    "business_df=business_df.drop_duplicates()\n",
    "#business_df.to_csv('business.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we look in the computer science domain\n",
    "We perform the same operation in the computer science domain, and we create a second df, with the same structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize doc search object using Scopus and execute search, retrieving \n",
    "#   all results\n",
    "comp_df=pd.DataFrame(columns=['Author(s)', 'Title', 'YearPublished', 'DOI','ScopusID'])\n",
    "\n",
    "for key in keyword_list:\n",
    "    #we print the keyword as an indicator of the progress\n",
    "    #print(key)\n",
    "    doc_srch = ElsSearch(\"TITLE-ABS-KEY(\"+key+\") AND SUBJAREA(COMP)\",'scopus')\n",
    "    doc_srch.execute(client, get_all = True)\n",
    "\n",
    "    for el in doc_srch.results:\n",
    "        try:\n",
    "            data=[el[\"dc:creator\"],el[\"dc:title\"],el[\"prism:coverDate\"], el[\"prism:doi\"], el[\"dc:identifier\"].split(\":\")[1]]\n",
    "        except Exception as e:\n",
    "            data=[]\n",
    "            input_elements=[\"dc:creator\", \"dc:title\", \"prism:coverDate\", \"prism:doi\", \"dc:identifier\"]\n",
    "            for field in input_elements:\n",
    "                try:\n",
    "                    data.append(el[field])\n",
    "                except Exception as f:\n",
    "                    data.append(\"NaN\")\n",
    "\n",
    "        comp_df.loc[len(comp_df), :] = data\n",
    "\n",
    "comp_df=comp_df.drop_duplicates()\n",
    "#comp_df.to_csv('computerScience.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now look for papers found in both categories\n",
    "We will start by reviewing them, and then we will proceed with single domains and eliminate papers whose titles clearly do not fit with our research questions. <br>\n",
    "We will then <b>scrape abstracts</b> for the remaining papers and further refine our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df=pd.read_csv('business.csv') \n",
    "comp_df=pd.read_csv('computerScience.csv') \n",
    "common_papers = pd.merge(comp_df, business_df, on=[\"Author(s)\", \"Title\", \"YearPublished\", \"DOI\", \"ScopusID\"])\n",
    "common_papers=common_papers.drop_duplicates()\n",
    "all_papers = pd.concat([business_df, comp_df])\n",
    "all_papers = all_papers.drop_duplicates()\n",
    "\n",
    "#all_papers.to_csv('allPapers.csv', index=False)\n",
    "#common_papers.to_csv('commonPapers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After retrieving papers we proceeded to manually clean them\n",
    "We looked at titles and checked for their relevance given the topic of our review. the next step is to create single 'clean' files for each of our subjects (i.e., Busness and Computer Science). To do so, we will simply compare the single documents with the cleaned one and remove papers that do not belong to their intersection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#we read all the datasets\n",
    "business_df=pd.read_csv('business.csv') \n",
    "comp_df=pd.read_csv('computerScience.csv') \n",
    "common_papers = pd.read_csv('commonPapers.csv') \n",
    "all_papers = pd.read_csv('clean/allPapersClean.csv')\n",
    "\n",
    "business_df_clean=pd.merge(all_papers, business_df[\"DOI\"], how ='inner', on=[\"DOI\"])\n",
    "comp_df_clean=pd.merge(all_papers, comp_df[\"DOI\"], how ='inner', on=[\"DOI\"])\n",
    "common_papers_clean=pd.merge(all_papers, common_papers[\"DOI\"], how ='inner', on=[\"DOI\"])\n",
    "\n",
    "#business_df_clean.to_csv(\"clean/businessClean.csv\", index=False)\n",
    "#comp_df_clean.to_csv(\"clean/computerScienceClean.csv\", index=False)\n",
    "#common_papers_clean.to_csv(\"clean/commonPapersClean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now look at citations of each of the papers we retrieved\n",
    "To do so we will use Crossref API through the DOIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from habanero import Crossref\n",
    "from habanero import counts\n",
    "from habanero import cn\n",
    "cr = Crossref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notFound=[]\n",
    "references=[]\n",
    "citations_dict={}\n",
    "\n",
    "business_df_clean=pd.read_csv('clean/businessClean.csv')\n",
    "start_year=business_df_clean[\"YearPublished\"].min().split(\"/\")[2]\n",
    "end_year=business_df_clean[\"YearPublished\"].max().split(\"/\")[2]\n",
    "\n",
    "business_citations_edges=pd.DataFrame(columns=['Source', 'Target'])\n",
    "business_citations_nodes=pd.DataFrame(columns=['Key', 'Title', 'PubYear'])\n",
    "\n",
    "for row in business_df_clean.iterrows():\n",
    "    id=str(row[1][\"DOI\"])\n",
    "    try:\n",
    "        references=cr.works(ids = id)[\"message\"][\"reference\"]\n",
    "        business_citations_nodes.loc[len(business_citations_nodes), :] = [id,cr.works(ids = id)[\"message\"][\"title\"][0], cr.works(ids = id)[\"message\"][\"created\"][\"date-parts\"][0][0]]\n",
    "        reference_list=[]\n",
    "        for item in references:\n",
    "            data=[item[\"key\"]]\n",
    "            business_citations_edges.loc[len(business_citations_edges), :] = [id,item[\"key\"]]\n",
    "            if \"article-title\" in item.keys():\n",
    "                data.append(item[\"article-title\"])\n",
    "            else:\n",
    "                data.append(\"NoTitle\")\n",
    "            if \"year\" in item.keys():\n",
    "                data.append(item[\"year\"])\n",
    "            else:\n",
    "                data.append(\"NoDate\")\n",
    "            business_citations_nodes.loc[len(business_citations_nodes), :] = data\n",
    "    except Exception as e:\n",
    "        notFound.append(str(e).split(\"/\")[-1])                \n",
    "\n",
    "business_citations_edges=business_citations_edges.drop_duplicates()\n",
    "business_citations_nodes=business_citations_nodes.drop_duplicates()\n",
    "\n",
    "#business_citations_nodes.to_csv(\"Networks/businessCitationsNodes.csv\", index=False)\n",
    "#business_citations_edges.to_csv(\"Networks/businessCitationsEdges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the same procedure for <b>Computer Science</b> papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notFound=[]\n",
    "references=[]\n",
    "citations_dict={}\n",
    "\n",
    "comp_df_clean=pd.read_csv('clean/computerScienceClean.csv')\n",
    "start_year=comp_df_clean[\"YearPublished\"].min().split(\"/\")[2]\n",
    "end_year=comp_df_clean[\"YearPublished\"].max().split(\"/\")[2]\n",
    "\n",
    "comp_citations_edges=pd.DataFrame(columns=['Source', 'Target'])\n",
    "comp_citations_nodes=pd.DataFrame(columns=['Key', 'Title', 'PubYear'])\n",
    "\n",
    "for row in comp_df_clean.iterrows():\n",
    "    id=str(row[1][\"DOI\"])\n",
    "    try:\n",
    "        references=cr.works(ids = id)[\"message\"][\"reference\"]\n",
    "        comp_citations_nodes.loc[len(comp_citations_nodes), :] = [id,cr.works(ids = id)[\"message\"][\"title\"][0], cr.works(ids = id)[\"message\"][\"created\"][\"date-parts\"][0][0]]\n",
    "        reference_list=[]\n",
    "        for item in references:\n",
    "            data=[item[\"key\"]]\n",
    "            comp_citations_edges.loc[len(comp_citations_edges), :] = [id,item[\"key\"]]\n",
    "            if \"article-title\" in item.keys():\n",
    "                data.append(item[\"article-title\"])\n",
    "            else:\n",
    "                data.append(\"NoTitle\")\n",
    "            if \"year\" in item.keys():\n",
    "                data.append(item[\"year\"])\n",
    "            else:\n",
    "                data.append(\"NoDate\")\n",
    "            comp_citations_nodes.loc[len(comp_citations_nodes), :] = data\n",
    "    except Exception as e:\n",
    "        notFound.append(str(e).split(\"/\")[-1])               \n",
    "\n",
    "comp_citations_edges=comp_citations_edges.drop_duplicates()\n",
    "comp_citations_nodes=comp_citations_nodes.drop_duplicates()\n",
    "\n",
    "#comp_citations_nodes.to_csv(\"Networks/compCitationsNodes.csv\", index=False)\n",
    "#comp_citations_edges.to_csv(\"Networks/compCitationsEdges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-citations...\n",
    "From the citation network we just obtained we can deduce co-citations and bibliographic coupling networks.<br>\n",
    "We will start with the <b>co-citation</b> network for <b>Business</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "business_citations_edges=pd.read_csv('Networks/businessCitationsEdges.csv')\n",
    "business_citations_nodes=pd.read_csv('Networks/businessCitationsNodes.csv')\n",
    "\n",
    "edges=[]\n",
    "business_co_citations_nodes=pd.DataFrame(columns=['Key', 'Title', 'PubYear'])\n",
    "done_ids=[]\n",
    "\n",
    "#first we find co-citations\n",
    "for row in business_citations_edges.iterrows():\n",
    "    if row[1][\"Source\"] not in done_ids:\n",
    "        paper_citations=business_citations_edges.loc[business_citations_edges['Source'].isin([row[1][\"Source\"]])]\n",
    "        idx=paper_citations.index\n",
    "        c=idx[0]\n",
    "        while c<=idx[-1]:\n",
    "            business_co_citations_nodes=pd.concat([business_co_citations_nodes, (business_citations_nodes[(business_citations_nodes['Key'] == paper_citations.loc[c][\"Target\"])])])\n",
    "            g=c+1\n",
    "            while g<=idx[-1]:\n",
    "                edges.append((paper_citations.loc[c][\"Target\"],paper_citations.loc[g][\"Target\"]))\n",
    "                                                \n",
    "                business_co_citations_nodes=pd.concat([business_co_citations_nodes, (business_citations_nodes[(business_citations_nodes['Key'] == paper_citations.loc[g][\"Target\"])])])\n",
    "\n",
    "                g+=1\n",
    "\n",
    "            c+=1\n",
    "        done_ids.append(row[1][\"Source\"])\n",
    "\n",
    "'''\n",
    "business_co_citations_nodes=business_co_citations_nodes.drop_duplicates()\n",
    "business_co_citations_nodes.to_csv(\"Networks/businessCoCitationsNodes.csv\", index=False)\n",
    "\n",
    "# Configures output path\n",
    "output_path = \"Networks/businessCoCitationsEdges.csv\"\n",
    "\n",
    "# Writes synonyms to specified output\n",
    "with open(output_path, mode=\"w\", encoding=\"UTF-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "    writer.writerow([\"Source\", \"Target\"])\n",
    "    for row in edges:\n",
    "        buffer = [[source, target] for source in row for target in row if source != target]\n",
    "        writer.writerows(buffer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also extract our <b>co-citation</b> network for <b>Computer science</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "comp_citations_edges=pd.read_csv('Networks/compCitationsEdges.csv')\n",
    "comp_citations_nodes=pd.read_csv('Networks/compCitationsNodes.csv')\n",
    "\n",
    "edges=[]\n",
    "comp_co_citations_nodes=pd.DataFrame(columns=['Key', 'Title', 'PubYear'])\n",
    "done_ids=[]\n",
    "\n",
    "#first we find co-citations\n",
    "for row in comp_citations_edges.iterrows():\n",
    "    if row[1][\"Source\"] not in done_ids:\n",
    "        paper_citations=comp_citations_edges.loc[comp_citations_edges['Source'].isin([row[1][\"Source\"]])]\n",
    "        idx=paper_citations.index\n",
    "        c=idx[0]\n",
    "        while c<=idx[-1]:\n",
    "            comp_co_citations_nodes=pd.concat([comp_co_citations_nodes, (comp_citations_nodes[(comp_citations_nodes['Key'] == paper_citations.loc[c][\"Target\"])])])\n",
    "            g=c+1\n",
    "            while g<=idx[-1]:\n",
    "                edges.append((paper_citations.loc[c][\"Target\"],paper_citations.loc[g][\"Target\"]))\n",
    "                                                \n",
    "                comp_co_citations_nodes=pd.concat([comp_co_citations_nodes, (comp_citations_nodes[(comp_citations_nodes['Key'] == paper_citations.loc[g][\"Target\"])])])\n",
    "\n",
    "                g+=1\n",
    "\n",
    "            c+=1\n",
    "        done_ids.append(row[1][\"Source\"])\n",
    "\n",
    "\n",
    "'''\n",
    "comp_co_citations_nodes=comp_co_citations_nodes.drop_duplicates()\n",
    "comp_co_citations_nodes.to_csv(\"Networks/compCoCitationsNodes.csv\", index=False)\n",
    "\n",
    "# Configures output path\n",
    "output_path = \"Networks/compCoCitationsEdges.csv\"\n",
    "\n",
    "# Writes synonyms to specified output\n",
    "with open(output_path, mode=\"w\", encoding=\"UTF-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "    writer.writerow([\"Source\", \"Target\"])\n",
    "    for row in edges:\n",
    "        buffer = [[source, target] for source in row for target in row if source != target]\n",
    "        writer.writerows(buffer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...and bibliographic coupling\n",
    "We then proceed to check for <b>Bibliographic coupling</b> for <b>Business</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "business_citations_edges=pd.read_csv('Networks/businessCitationsEdges.csv')\n",
    "business_citations_edges=business_citations_edges.sort_values(by=['Target'], ignore_index=True)\n",
    "business_citations_nodes=pd.read_csv('Networks/businessCitationsNodes.csv')\n",
    "\n",
    "edges=[]\n",
    "business_bib_coupling_nodes=pd.DataFrame(columns=['Key', 'Title', 'PubYear'])\n",
    "done_ids=[]\n",
    "\n",
    "#then we check for bibliographic coupling\n",
    "for row in business_citations_edges.iterrows():\n",
    "    if row[1][\"Target\"] not in done_ids:\n",
    "        paper_citations=business_citations_edges.loc[business_citations_edges['Target'].isin([row[1][\"Target\"]])]\n",
    "        idx=paper_citations.index\n",
    "        c=idx[0]\n",
    "        while c<=idx[-1]:\n",
    "            business_bib_coupling_nodes=pd.concat([business_bib_coupling_nodes, (business_citations_nodes[(business_citations_nodes['Key'] == paper_citations.loc[c][\"Source\"])])])\n",
    "            g=c+1\n",
    "            while g<=idx[-1]:\n",
    "                edges.append((paper_citations.loc[c][\"Source\"],paper_citations.loc[g][\"Source\"]))\n",
    "                                                \n",
    "                business_bib_coupling_nodes=pd.concat([business_bib_coupling_nodes, (business_citations_nodes[(business_citations_nodes['Key'] == paper_citations.loc[g][\"Source\"])])])\n",
    "\n",
    "                g+=1\n",
    "\n",
    "            c+=1\n",
    "        done_ids.append(row[1][\"Target\"])\n",
    "\n",
    "\n",
    "'''\n",
    "business_bib_coupling_nodes=business_bib_coupling_nodes.drop_duplicates()\n",
    "business_bib_coupling_nodes.to_csv(\"Networks/businessBibCouplingNodes.csv\", index=False)\n",
    "\n",
    "# Configures output path\n",
    "output_path = \"Networks/businessBibCouplingEdges.csv\"\n",
    "\n",
    "# Writes synonyms to specified output\n",
    "with open(output_path, mode=\"w\", encoding=\"UTF-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "    writer.writerow([\"Source\", \"Target\"])\n",
    "    for row in edges:\n",
    "        buffer = [[source, target] for source in row for target in row if source != target]\n",
    "        writer.writerows(buffer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our <b>Bibliographic coupling</b> network for <b>Computer Science</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "comp_citations_edges=pd.read_csv('Networks/compCitationsEdges.csv')\n",
    "comp_citations_edges=comp_citations_edges.sort_values(by=['Target'], ignore_index=True)\n",
    "comp_citations_nodes=pd.read_csv('Networks/compCitationsNodes.csv')\n",
    "\n",
    "edges=[]\n",
    "comp_bib_coupling_nodes=pd.DataFrame(columns=['Key', 'Title', 'PubYear'])\n",
    "done_ids=[]\n",
    "\n",
    "#then we check for bibliographic coupling\n",
    "for row in comp_citations_edges.iterrows():\n",
    "    if row[1][\"Target\"] not in done_ids:\n",
    "        paper_citations=comp_citations_edges.loc[comp_citations_edges['Target'].isin([row[1][\"Target\"]])]\n",
    "        idx=paper_citations.index\n",
    "        c=idx[0]\n",
    "        while c<=idx[-1]:\n",
    "            comp_bib_coupling_nodes=pd.concat([comp_bib_coupling_nodes, (comp_citations_nodes[(comp_citations_nodes['Key'] == paper_citations.loc[c][\"Source\"])])])\n",
    "            g=c+1\n",
    "            while g<=idx[-1]:\n",
    "                edges.append((paper_citations.loc[c][\"Source\"],paper_citations.loc[g][\"Source\"]))\n",
    "                                                \n",
    "                comp_bib_coupling_nodes=pd.concat([comp_bib_coupling_nodes, (comp_citations_nodes[(comp_citations_nodes['Key'] == paper_citations.loc[g][\"Source\"])])])\n",
    "\n",
    "                g+=1\n",
    "\n",
    "            c+=1\n",
    "        done_ids.append(row[1][\"Target\"])\n",
    "\n",
    "'''\n",
    "comp_bib_coupling_nodes=comp_bib_coupling_nodes.drop_duplicates()\n",
    "comp_bib_coupling_nodes.to_csv(\"Networks/compBibCouplingNodes.csv\", index=False)\n",
    "\n",
    "# Configures output path\n",
    "output_path = \"Networks/compBibCouplingEdges.csv\"\n",
    "\n",
    "# Writes synonyms to specified output\n",
    "with open(output_path, mode=\"w\", encoding=\"UTF-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "    writer.writerow([\"Source\", \"Target\"])\n",
    "    for row in edges:\n",
    "        buffer = [[source, target] for source in row for target in row if source != target]\n",
    "        writer.writerows(buffer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now merge the graphs and compute our measures regarding boundary papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#business citations\n",
    "business_citations_edges=pd.read_csv('Networks/businessCitationsEdges.csv')\n",
    "business_citations_nodes=pd.read_csv('Networks/businessCitationsNodes.csv')\n",
    "#computer science citations\n",
    "comp_citations_edges=pd.read_csv('Networks/compCitationsEdges.csv')\n",
    "comp_citations_nodes=pd.read_csv('Networks/compCitationsNodes.csv')\n",
    "#intersection citations\n",
    "intersection_citations=pd.merge(comp_citations_edges, business_citations_edges, how ='inner', on=[\"Source\", \"Target\"])\n",
    "intersection_nodes=set()\n",
    "for row in intersection_citations.iterrows():\n",
    "    intersection_nodes.add(row[1][\"Source\"])\n",
    "    intersection_nodes.add(row[1][\"Target\"])\n",
    "#merge citations\n",
    "merge_citations=pd.concat([business_citations_edges, comp_citations_edges]).drop_duplicates()\n",
    "merge_nodes=set()\n",
    "for row in merge_citations.iterrows():\n",
    "    merge_nodes.add(row[1][\"Source\"])\n",
    "    merge_nodes.add(row[1][\"Target\"])\n",
    "\n",
    "cross_disc_cits=set()\n",
    "for row in business_citations_edges.iterrows():\n",
    "    if row[1][\"Source\"] in comp_citations_nodes['Id'].values or row[1][\"Target\"] in comp_citations_nodes['Id'].values:\n",
    "        for row_2 in intersection_citations.iterrows():\n",
    "            if (row[1][\"Source\"] == row_2[1][\"Source\"] and row[1][\"Target\"] == row_2[1][\"Target\"]) or (row[1][\"Target\"] == row_2[1][\"Source\"] and row[1][\"Source\"] == row_2[1][\"Target\"]):\n",
    "                continue\n",
    "            else:\n",
    "               cross_disc_cits.add((row[1][\"Source\"], row[1][\"Target\"])) \n",
    "\n",
    "for row in comp_citations_edges.iterrows():\n",
    "    if row[1][\"Source\"] in business_citations_nodes['Id'].values or row[1][\"Target\"] in business_citations_nodes['Id'].values:\n",
    "        for row_2 in intersection_citations.iterrows():\n",
    "            if (row[1][\"Source\"] == row_2[1][\"Source\"] and row[1][\"Target\"] == row_2[1][\"Target\"]) or (row[1][\"Target\"] == row_2[1][\"Source\"] and row[1][\"Source\"] == row_2[1][\"Target\"]):\n",
    "                continue\n",
    "            else:\n",
    "               cross_disc_cits.add((row[1][\"Source\"], row[1][\"Target\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2617953060730704 0.6169976289270895 1.9397992447619322e-05\n"
     ]
    }
   ],
   "source": [
    "boundary_papers=len(intersection_nodes)/len(merge_nodes)\n",
    "citation_connections=len(comp_citations_edges.index)+len(business_citations_edges.index)-len(merge_citations.index)+len(cross_disc_cits)\n",
    "cross_disc=(1-(len(comp_citations_edges.index)+len(business_citations_edges.index)-len(merge_citations.index))/citation_connections)\n",
    "dominance_level=boundary_papers/citation_connections\n",
    "print(boundary_papers, cross_disc, dominance_level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
